{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cd0224-ab1c-4c58-a566-bf4d5c10e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi output:\n",
      "\n",
      "Thu Nov  7 08:21:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:17:00.0 Off |                    0 |\n",
      "|  0%   55C    P0             87W /  300W |    3717MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A40                     On  |   00000000:65:00.0 Off |                    0 |\n",
      "|  0%   56C    P0             88W /  300W |   44295MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A40                     On  |   00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   31C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A40                     On  |   00000000:E3:00.0 Off |                    0 |\n",
      "|  0%   31C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     96617      C   ...10/models_citation/myenv/bin/python       3708MiB |\n",
      "|    1   N/A  N/A   1230721      C   /home/historynlp/venv/bin/python3            1414MiB |\n",
      "|    1   N/A  N/A   2184881      C   /home/historynlp/venv/bin/python3           42866MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        # Run the nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(\"nvidia-smi output:\\n\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"Error running nvidia-smi:\\n\")\n",
    "            print(result.stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to run nvidia-smi\n",
    "run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55967953-e9ae-4228-8de2-fadde8dced44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartinez1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/smartinez1/.venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2024-11-06 23:54:20.713072: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 23:54:20.892774: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 23:54:21.532203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-06 23:54:21.532271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-06 23:54:21.532277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/smartinez1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Specify the GPU index you want to check\n",
    "gpu_index = 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_index}\"\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc36574-f284-4c1c-8578-06dd259ee956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "\n",
      "Device 0: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "    \n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"\\nDevice {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7426075-d289-43ba-9605-4a6bfd05f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[1]\n",
    "TOKEN = '' \n",
    "login(token=TOKEN)\n",
    "\n",
    "# CHECKPOINT_PATH = \"Llama-3.1-8B_2\" # Define if you want to start training from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9695f43f-1050-48b0-a7e3-dfc6ebfa0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525b4b8e-48f1-4367-a730-e0cda9779220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer with quantization settings\n",
    "if not CHECKPOINT_PATH:\n",
    "    model_q = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "else:\n",
    "    model_q = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7fb978-e06e-417e-842a-d7dc47f9d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", 'k_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model_q = get_peft_model(model_q, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571bad4-245b-4f60-9e9f-d1765d7fe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "with open('corpus_v5.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334274c0-8f46-4078-a5d8-0e727792fb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc0f881-04b9-42d9-b825-138db8428140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avis juridique important\n",
      "\n",
      " \n",
      "Corrigendum to Council Regulation (EC, Euratom) No 1605/2002 of 25 June 2002 on the Financial Regulation applicable to the general budget of the European Communities (OJ L 248 of 16.9.2002)  \n",
      "\n",
      "\n",
      "Official Journal L 025 , 30/01/2003 P. 0043 - 0043 \n",
      " \n",
      "\n",
      "Corrigendum to Council Regulation (EC, Euratom) No 1605/2002 of 25 June 2002 on the Financial Regulation applicable to the general budget of the European Communities(Official Journal of the European Communities L 248 of 16 September 2002)On page 21, in Article 66(1), the final sentence:for: \"The same shall apply where, through serious misconduct, he/she omits to draw up a document establishing a debt of if he/she neglects to issue a recovery order or is, without justification, late in issuing it, as the issuing of a payment order may involve third-party liability of the institution.\",read: \"The same shall apply where, through serious misconduct, he/she fails to draw up a document establishing an amount receivable or if he/she fails to issue a recovery order or is, without justification, late in issuing it, or if he/she fails to issue a payment order or is, without justification, late in issuing it, thereby rendering the institution liable to civil action by third parties.\";on page 34, in Article 144(2):for: 2. The opinions referred to in Article 248(4) of the EC Treaty and Article 180a(4) of Euratom Treaty ...,read: 2. The opinions referred to in Article 248(4) of the EC Treaty and Article 160c(4) of the Euratom Treaty ...;on page 40, in Article 179(1):for: 1. Administrative appropriations shall be differentiated appropriations.,read: 1. Administrative appropriations shall be non-differentiated appropriations.;on page 41, in Article 184: delete the second paragraph which reads as follows:\"Any regulations amending this Regulation shall be adopted by the Council after recourse to the conciliation procedure, if the European Parliament so requests.\" \n",
      "\n",
      " Corrigendum to Council Regulation (EC, Euratom) No 1605/2002 of 25 June 2002 on the Financial Regulation applicable to the general budget of the European Communities (Official Journal of the European Communities L 248 of 16 September 2002)  On page 21, in Article 66(1), the final sentence: for: \"The same shall apply where, through serious misconduct, he/she omits to draw up a document establishing a debt of if he/she neglects to issue a recovery order or is, without justification, late in issuing it, as the issuing of a payment order may involve third-party liability of the institution.\", read: \"The same shall apply where, through serious misconduct, he/she fails to draw up a document establishing an amount receivable or if he/she fails to issue a recovery order or is, without justification, late in issuing it, or if he/she fails to issue a payment order or is, without justification, late in issuing it, thereby rendering the institution liable to civil action by third parties.\"; on page 34, in Article 144(2): for:  2. The opinions referred to in Article 248(4) of the EC Treaty and Article 180a(4) of Euratom Treaty ..., read:  2. The opinions referred to in Article 248(4) of the EC Treaty and Article 160c(4) of the Euratom Treaty ...; on page 40, in Article 179(1): for:  1. Administrative appropriations shall be differentiated appropriations., read:  1. Administrative appropriations shall be non-differentiated appropriations.; on page 41, in Article 184: delete the second paragraph which reads as follows:\"Any regulations amending this Regulation shall be adopted by the Council after recourse to the conciliation procedure, if the European Parliament so requests.\"   \n"
     ]
    }
   ],
   "source": [
    "print(dataset[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9644aa-b4da-4430-b185-a62b51cc360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [item for item in dataset if str(item) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058f2635-5cbb-4214-a130-f4406f26a429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa42ef8-e5fd-4a16-8851-b4b7f6fed22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch LLaMA's start and end tokens\n",
    "end_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07b5010-a2b1-486a-ba76-4528d57fb815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128001\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6dd6969-1531-48e3-801c-32d5aab48792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a unique padding token for fine-tuning\n",
    "custom_pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.add_special_tokens({'pad_token': custom_pad_token})\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "max_length = 128  # Maximum tokens per batch\n",
    "\n",
    "# Define a preprocessing function for phrases with token accumulation\n",
    "def preprocess_and_accumulate_phrases(document, max_length):\n",
    "    phrases = nltk.sent_tokenize(document)\n",
    "    accumulated_tokens = []\n",
    "    processed_batches = []\n",
    "    ignored_phrases_count = 0\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        if len(phrase) < 50:  # Skip short phrases\n",
    "            ignored_phrases_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Tokenize the phrase and add the end token\n",
    "        encoding = tokenizer(phrase, return_tensors='pt', truncation=True, max_length=max_length - 1)\n",
    "        input_ids = encoding['input_ids'].squeeze().tolist() + [end_token_id]\n",
    "        \n",
    "        # Check if adding this phrase would exceed the max token length\n",
    "        if len(accumulated_tokens) + len(input_ids) > max_length:\n",
    "            # If it does, save the accumulated batch and start a new one\n",
    "            accumulated_tokens += [pad_token_id] * (max_length - len(accumulated_tokens))  # Pad to max_length\n",
    "            processed_batches.append({\n",
    "                'input_ids': torch.tensor(accumulated_tokens),  \n",
    "                'labels': torch.tensor(accumulated_tokens)    \n",
    "            })\n",
    "            accumulated_tokens = input_ids  # Start new batch with current phrase\n",
    "        else:\n",
    "            accumulated_tokens.extend(input_ids)  # Append current phrase\n",
    "    \n",
    "    # Add the last accumulated batch if it has tokens\n",
    "    if accumulated_tokens:\n",
    "        accumulated_tokens += [pad_token_id] * (max_length - len(accumulated_tokens))\n",
    "        processed_batches.append({\n",
    "            'input_ids': torch.tensor(accumulated_tokens), \n",
    "            'labels': torch.tensor(accumulated_tokens)    \n",
    "        })\n",
    "\n",
    "    return processed_batches, ignored_phrases_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c08e97-9af9-4e02-a39f-27f6dd48ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset from processed_filtered_dataset.pkl...\n",
      "Loaded processed dataset with 242316 batches.\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the processed dataset\n",
    "output_file_path = \"processed_filtered_dataset.pkl\"\n",
    "\n",
    "# Check if the processed file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    print(f\"Loading processed dataset from {output_file_path}...\")\n",
    "    with open(output_file_path, 'rb') as f:\n",
    "        processed_dataset = pickle.load(f)\n",
    "    print(f\"Loaded processed dataset with {len(processed_dataset)} batches.\")\n",
    "else:\n",
    "    # Start timer for processing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize variables for the processing loop\n",
    "    processed_dataset = []\n",
    "    total_ignored_phrases = 0\n",
    "\n",
    "    # Process each document in the dataset\n",
    "    for doc in tqdm(dataset, desc=\"Processing Dataset\"):\n",
    "        try:\n",
    "            batches, ignored_count = preprocess_and_accumulate_phrases(doc, max_length)\n",
    "        except TypeError as e:\n",
    "            print(f\"document {doc}\")\n",
    "            print(e)\n",
    "        processed_dataset.extend(batches)\n",
    "        total_ignored_phrases += ignored_count\n",
    "\n",
    "    # Calculate processing time and output statistics\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Total processed batches: {len(processed_dataset)}\")\n",
    "    print(f\"Total ignored phrases: {total_ignored_phrases}\")\n",
    "\n",
    "    # Save the processed dataset to a pickle file\n",
    "    with open(output_file_path, 'wb') as f:\n",
    "        pickle.dump(processed_dataset, f)\n",
    "    print(f\"Filtered dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009f34c3-a4c9-400d-b58c-dff2d05b548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for training\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # Keep data as is\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the item directly without moving to CPU\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba19210-649e-4efc-8b21-eed327a008c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation datasets\n",
    "random.seed(42)\n",
    "random.shuffle(processed_dataset)\n",
    "train_examples = round(len(processed_dataset) * 0.95)\n",
    "\n",
    "# Pass the device to the CustomDataset constructor\n",
    "train_dataset = CustomDataset(processed_dataset[:train_examples])\n",
    "val_dataset = CustomDataset(processed_dataset[train_examples:])\n",
    "\n",
    "training_epochs = 3\n",
    "model_dir = f\"{MODEL_NAME}_{training_epochs}\"\n",
    "\n",
    "# Set up the trainer with causal language modeling and modified arguments\n",
    "trainer = Trainer(\n",
    "    model=model_q,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=28,\n",
    "        per_device_eval_batch_size=20,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=500,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_8bit\",\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=training_epochs,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        output_dir=model_dir,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda656ba-f696-463a-81ce-e59fe30b345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2055' max='2055' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2055/2055 4:28:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.769300</td>\n",
       "      <td>2.781334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.721100</td>\n",
       "      <td>2.711690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.700300</td>\n",
       "      <td>2.677999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.660600</td>\n",
       "      <td>2.662708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartinez1/.venv/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (Request ID: Root=1-672ba1af-6708579754f3381c24fc1fd7;8c84b238-1904-4799-a5f9-06c9b93ec965)\n",
      "\n",
      "403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\n",
      "Cannot access content at: https://huggingface.co/meta-llama/Llama-3.1-8B/resolve/main/config.json.\n",
      "Make sure your token has the correct permissions. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B.\n",
      "  warnings.warn(\n",
      "/home/smartinez1/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 16104.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B_1/tokenizer_config.json',\n",
       " 'Llama-3.1-8B_1/special_tokens_map.json',\n",
       " 'Llama-3.1-8B_1/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "total_time = time.time() - start_time\n",
    "# Print the total processing time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model_q.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
