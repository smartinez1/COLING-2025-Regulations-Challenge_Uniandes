{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cd0224-ab1c-4c58-a566-bf4d5c10e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi output:\n",
      "\n",
      "Fri Nov  8 21:08:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:17:00.0 Off |                    0 |\n",
      "|  0%   52C    P0             85W /  300W |    3717MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A40                     On  |   00000000:65:00.0 Off |                    0 |\n",
      "|  0%   54C    P0             87W /  300W |    3195MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A40                     On  |   00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   36C    P8             24W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A40                     On  |   00000000:E3:00.0 Off |                    0 |\n",
      "|  0%   48C    P0             61W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     96617      C   ...10/models_citation/myenv/bin/python       3708MiB |\n",
      "|    1   N/A  N/A   1230721      C   /home/historynlp/venv/bin/python3            1414MiB |\n",
      "|    1   N/A  N/A   2212129      C   /home/historynlp/venv/bin/python3            1766MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        # Run the nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(\"nvidia-smi output:\\n\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"Error running nvidia-smi:\\n\")\n",
    "            print(result.stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to run nvidia-smi\n",
    "run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55967953-e9ae-4228-8de2-fadde8dced44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartinez1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/smartinez1/.venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2024-11-08 21:08:54.685485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 21:08:54.840605: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-08 21:08:55.450512: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-08 21:08:55.450571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-08 21:08:55.450577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Specify the GPU index you want to check\n",
    "gpu_index = 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_index}\"\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc36574-f284-4c1c-8578-06dd259ee956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "\n",
      "Device 0: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "    \n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"\\nDevice {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7426075-d289-43ba-9605-4a6bfd05f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[1]\n",
    "TOKEN = ''\n",
    "login(token=TOKEN)\n",
    "\n",
    "CHECKPOINT_PATH = \"Llama-3.1-8B_1\" # Define lllama model we pretrained on full corpus\n",
    "training_epochs = 3\n",
    "model_dir = f\"{MODEL_NAME}_instruct_{training_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3bfa97-2be7-410d-9787-fa5c148f9432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-3.1-8B_instruct_3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9695f43f-1050-48b0-a7e3-dfc6ebfa0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525b4b8e-48f1-4367-a730-e0cda9779220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer with quantization settings\n",
    "if not CHECKPOINT_PATH:\n",
    "    model_q = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "else:\n",
    "    model_q = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7fb978-e06e-417e-842a-d7dc47f9d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", 'k_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model_q = get_peft_model(model_q, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7571bad4-245b-4f60-9e9f-d1765d7fe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_dict(json_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load a JSON file into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary representation of the JSON data.\n",
    "    \"\"\"\n",
    "    json_path = os.path.join(json_file)\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "consolidated_data = load_json_to_dict(\"consolidated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e45d0c5-670c-4a60-9615-740bf73600de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Expand the following acronym into its full form: {}',\n",
       " 'input': 'DDP',\n",
       " 'output': 'Data Download Program'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_data[14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "334274c0-8f46-4078-a5d8-0e727792fb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16615"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(consolidated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fc0f881-04b9-42d9-b825-138db8428140",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4ffb53-6da9-4d57-8a2c-2af75ade5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign eos_token as pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Format examples and tokenize\n",
    "def format_prompt(entry, print_example=False):\n",
    "    # Populate the instruction template with entry input\n",
    "    instruction = entry['instruction'].format(entry['input'])\n",
    "    answer = entry['output']\n",
    "    # Apply the full prompt template\n",
    "    context = prompt_template.format(instruction, answer)\n",
    "\n",
    "    # Optionally print for debugging or verification\n",
    "    if print_example:\n",
    "        print(context)\n",
    "        \n",
    "    # Tokenize with padding and truncation\n",
    "    encoded = tokenizer(\n",
    "        tokenizer.bos_token + context,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'][0],\n",
    "        'attention_mask': encoded['attention_mask'][0]\n",
    "    }\n",
    "\n",
    "# Create dataset by applying the format function to each entry\n",
    "formatted_dataset = [format_prompt(entry) for entry in consolidated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca5ff3e-0bff-4a82-b105-67c12e98dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Provide a concise answer to the following question: How did the Federal Reserve's actions in 2003 and early 2004 reflect compliance with the Federal Reserve Act, particularly in terms of reporting to Congress?\n",
      "\n",
      "### Answer:\n",
      "The Federal Reserve submitted its Monetary Policy Report to Congress pursuant to section 2B of the Federal Reserve Act, ensuring compliance with legislative requirements for transparency and accountability regarding monetary policy decisions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([128000, 128000,  39314,    374,    459,   7754,    430,  16964,    264,\n",
       "           3465,     13,   9842,    264,   2077,    430,  36001,  45695,    279,\n",
       "           1715,    382,  14711,  30151,    512,  61524,    264,  64694,   4320,\n",
       "            311,    279,   2768,   3488,     25,   2650,   1550,    279,  12411,\n",
       "          25820,    596,   6299,    304,    220,   1049,     18,    323,   4216,\n",
       "            220,   1049,     19,   8881,   8907,    449,    279,  12411,  25820,\n",
       "           3298,     11,   8104,    304,   3878,    315,  13122,    311,   8151,\n",
       "           1980,  14711,  22559,    512,    791,  12411,  25820,  14976,   1202,\n",
       "          74214,  11216,   8423,    311,   8151,  33549,    311,   3857,    220,\n",
       "             17,     33,    315,    279,  12411,  25820,   3298,     11,  23391,\n",
       "           8907,    449,  27743,   8670,    369,  28330,    323,  39242,   9002,\n",
       "          33384,   4947,  11429,    627, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_prompt(consolidated_data[7895], print_example=True)  #12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a83936c-7041-4f08-913f-20fdede6a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class for the trainer\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        values = self.data[idx]\n",
    "        labels = values['input_ids'].clone()\n",
    "        # Mask instruction tokens\n",
    "        labels[values['attention_mask'] == 0] = -100\n",
    "        return {\n",
    "            'input_ids': values['input_ids'],\n",
    "            'attention_mask': values['attention_mask'],\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0815511-dbff-480f-af38-680c31ab2c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16615"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba19210-649e-4efc-8b21-eed327a008c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation datasets\n",
    "random.seed(42)\n",
    "random.shuffle(formatted_dataset)\n",
    "train_examples = int(0.95 * len(formatted_dataset))\n",
    "\n",
    "# Pass the device to the CustomDataset constructor\n",
    "train_dataset = CustomDataset(formatted_dataset[:train_examples])\n",
    "val_dataset = CustomDataset(formatted_dataset[train_examples:])\n",
    "\n",
    "# Set up the trainer with causal language modeling and modified arguments\n",
    "trainer = Trainer(\n",
    "    model=model_q,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=14,\n",
    "        per_device_eval_batch_size=8,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=50,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_8bit\",\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=training_epochs,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        output_dir=model_dir,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda656ba-f696-463a-81ce-e59fe30b345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 59:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>0.729234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.660641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>0.637586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>0.622174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.620900</td>\n",
       "      <td>0.615431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.607351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.601065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.596197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.592224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>0.590864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.585337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.587770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.586924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.584776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.581796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.580911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 3564.77 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama-3.1-8B_instruct_3/tokenizer_config.json',\n",
       " 'Llama-3.1-8B_instruct_3/special_tokens_map.json',\n",
       " 'Llama-3.1-8B_instruct_3/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "total_time = time.time() - start_time\n",
    "# Print the total processing time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model_q.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfd931e-c587-4ee4-9c9b-9b4356c73600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:09<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "model_ft = AutoModelForCausalLM.from_pretrained(model_dir, device_map='auto')\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e30a2e-a701-46d3-b385-43763084102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The full form of the acronym ESMA is the **Environmental Safety and Monitoring Agency**.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "# Set up the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model_q, tokenizer=tokenizer)\n",
    "\n",
    "# Simulated conversation\n",
    "user_input = \"Expand the following acronym into its full form: ESMA.\"\n",
    "\n",
    "#user_input = \"Provide a link for Credit Card Accountability Responsibility and Disclosure Act law.\"\n",
    "\n",
    "#user_input =  \"Define the following term: National Automated Clearing House Association.\"\n",
    "\n",
    "prompt = prompt_template.format(user_input, '')\n",
    "# Generate a response\n",
    "response = generator(prompt, max_length=200, num_return_sequences=1, do_sample=True)\n",
    "response_str = response[0]['generated_text'].split('### Answer:')[1].strip()\n",
    "cut_ind = response_str.find(\"#\")\n",
    "response_str = response_str[:cut_ind].strip() if cut_ind!=-1 else response_str\n",
    "print(\"AI:\", response_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22ee3d9c-0f10-4046-b3be-6f431cc14607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: European Securities and Markets Authority\n"
     ]
    }
   ],
   "source": [
    "# Set up the text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model_ft, tokenizer=tokenizer_ft)\n",
    "# Simulated conversation\n",
    "user_input = \"Expand the following acronym into its full form: ESMA.\"\n",
    "\n",
    "#user_input = \"Provide a link for Credit Card Accountability Responsibility and Disclosure Act law.\"\n",
    "\n",
    "#user_input =  \"Define the following term: National Automated Clearing House Association.\"\n",
    "\n",
    "prompt = prompt_template.format(user_input, '')#f\"User: {user_input}\\nAI:\"\n",
    "# Generate a response\n",
    "response = generator(prompt, max_length=200, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Display the AI's response\n",
    "response_str = response[0]['generated_text'].split('### Answer:')[1].strip()\n",
    "cut_ind = response_str.find(\"#\")\n",
    "response_str = response_str[:cut_ind].strip() if cut_ind!=-1 else response_str\n",
    "print(\"AI:\", response_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c5ec783-c68a-471a-b537-db0b0fbf5967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartinez1/.venv/lib/python3.10/site-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "adapter_model.safetensors: 100%|███████████| 54.6M/54.6M [00:03<00:00, 16.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/smartinez1/Llama-3.1-8B-FINLLM/commit/e2bab10efff80a8019f489d02016811a2ac786d6', commit_message='Upload LlamaForCausalLM', commit_description='', oid='e2bab10efff80a8019f489d02016811a2ac786d6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/smartinez1/Llama-3.1-8B-FINLLM', endpoint='https://huggingface.co', repo_type='model', repo_id='smartinez1/Llama-3.1-8B-FINLLM'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.push_to_hub(\"smartinez1/Llama-3.1-8B-FINLLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4329-aadd-4e74-8912-80633e8a2886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
