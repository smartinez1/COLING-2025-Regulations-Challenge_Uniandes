{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi output:\n",
      "\n",
      "Thu Nov  7 23:41:40 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:17:00.0 Off |                    0 |\n",
      "|  0%   52C    P0             86W /  300W |    3717MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A40                     On  |   00000000:65:00.0 Off |                    0 |\n",
      "|  0%   54C    P0             86W /  300W |    3195MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A40                     On  |   00000000:CA:00.0 Off |                    0 |\n",
      "|  0%   58C    P0             95W /  300W |     815MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A40                     On  |   00000000:E3:00.0 Off |                    0 |\n",
      "|  0%   54C    P0             88W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     96617      C   ...10/models_citation/myenv/bin/python       3708MiB |\n",
      "|    1   N/A  N/A   1230721      C   /home/historynlp/venv/bin/python3            1414MiB |\n",
      "|    1   N/A  N/A   2212129      C   /home/historynlp/venv/bin/python3            1766MiB |\n",
      "|    2   N/A  N/A   2212953      C   /home/imartinezm2/myenv/bin/python3           806MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_nvidia_smi():\n",
    "    try:\n",
    "        # Run the nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(\"nvidia-smi output:\\n\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"Error running nvidia-smi:\\n\")\n",
    "            print(result.stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to run nvidia-smi\n",
    "run_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_index = 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_index}\"\n",
    "from huggingface_hub import login\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CUDA devices: 1\n",
      "\n",
      "Device 0: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of available CUDA devices: {num_devices}\")\n",
    "    \n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"\\nDevice {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "# Specify the device (0 for GPU or -1 for CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:02<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(\"smartinez1/Llama-3.1-8B-FINLLM\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "model = PeftModel.from_pretrained(base_model, \"smartinez1/Llama-3.1-8B-FINLLM\")\n",
    "# Load the tokenizer associated with the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Provide a link for Credit Card Accountability Responsibility and Disclosure Act law.\n",
      "AI: Credit Card Accountability Responsibility and Disclosure Act: https://www.federalreserve.gov/publicinfo/files/201407-hfesurvey.pdf\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Define the following term: National Automated Clearing House Association.\n",
      "AI: A clearinghouse that provides services for the transfer of funds between financial institutions.\n",
      "--------------------------------------------------\n",
      "User: Expand the following acronym into its full form: ESMA.\n",
      "AI: European Securities and Markets Authority\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Set up the text generation pipeline with the PEFT model, specifying the device\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# List of user inputs\n",
    "user_inputs = [\n",
    "    \"Provide a link for Credit Card Accountability Responsibility and Disclosure Act law.\",\n",
    "    \"Define the following term: National Automated Clearing House Association.\",\n",
    "    \"Expand the following acronym into its full form: ESMA.\"\n",
    "]\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Loop over each user input and generate a response\n",
    "for user_input in user_inputs:\n",
    "    # Format the user input into the prompt\n",
    "    prompt = prompt_template.format(user_input)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    response = generator(prompt, max_length=200, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "    # Extract and clean up the AI's response\n",
    "    response_str = response[0]['generated_text'].split('### Answer:')[1].strip()\n",
    "    cut_ind = response_str.find(\"#\")  # Remove extra information after the response\n",
    "    response_str = response_str[:cut_ind].strip() if cut_ind != -1 else response_str\n",
    "\n",
    "    # Display the AI's response\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"AI: {response_str}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
